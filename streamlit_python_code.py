# -*- coding: utf-8 -*-
"""최대한_교내디지털경진대회.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GtQwa3bF0FFGT-JxiOoqFEBVUFnpvbFN

정리본

!!필독!!

코랩 환경에서는 코랩이 자동으로 버전을 맞추려하는 기능이 있기에 일부 라이브러리는 install한 뒤 세션을 다시 시작해주어야 함. 안그러면 라이브러리가 최신화된 상태로 진행되어 충돌이나 에러가 발생함.

아래 라이브러리는 무조건 순서대로 설치해야 라이브러리간의 충돌을 피할 수 있음

또한 한글 폰트를 다운받은 후에도 세션을 재시작해주어야만 함.

GPT : 그냥 무료면 리소스 자체가 없음
LLaMA : 무료면 플랫폼 용량을 살짝 넘어서 못담음
OPT-1 : 답변이 오긴 했는데 이미 죽어버린 모델인듯, 심지어 동접이 많으면 안됨

나머지 해야할 거 streamlit

------------------------------------------------
카카오톡 파일을 Kakao_chat.txt로 입력하면 /
huggingface_output.txt, persistent_case.html을 사용자에게 제공하는 그림
"""


import streamlit as st
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer
from sentence_transformers import SentenceTransformer, util
import sentence_transformers
similarity_model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')
import os
import re
import csv
import json
import emoji
import torch
import joblib
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import numpy as np
import spacy
from tqdm import tqdm
from pathlib import Path
from collections import defaultdict
from datetime import datetime, timedelta
from gensim import corpora
from gensim.models import LdaModel
from torch.utils.data import Dataset, DataLoader
import logging
import matplotlib.colors as colors
import huggingface_hub
import transformers
import plotly.graph_objects as go
import matplotlib
import requests


def extract_nouns(text):
    # 2글자 이상 한글 단어만 추출하는 정규식 기반 함수
    return re.findall(r'[\uac00-\ud7a3]{2,}', text)

class ChatParser:
    DATE_PATTERN = re.compile(r"--------------- (\d+)년 (\d+)월 (\d+)일 [월화수목금토일]요일 ---------------")
    SYSTEM_PATTERN = re.compile(r"(.+)님이 (.+)님(?:을|를) 초대했습니다\.")
    MESSAGE_PATTERN = re.compile(r"\[(.*?)\] \[(오전|오후) (\d{1,2}):(\d{2})\] (.+)")
    REACTIVE_PATTERNS = [
        r'\b(?:ㅇㅋ|ㅇㅇ|ㄱㄱ|ㄲㅂ)\b',
        r'\b(?:응|넵|네|예|예스|확인|알겠|알았)\b',
        r'\b(?:맞아|그래|동의|좋아|좋은|좋습|맞|맞음|맞네)\b',
        r'\b(?:감사|고마워|고맙|땡큐)\b',
        r'(?:ㅋㅋ|ㅎㅇ|ㅎㅎ|ㅠㅠ|ㅜㅜ)'
    ]
    REACTIVE_REGEX = [re.compile(p) for p in REACTIVE_PATTERNS]
    _nlp = None
    _nlp_checked = False

    def __init__(self):
        self.speakers = set()
        self.message_log = []
        self.invites = []
        self.current_date = None
        self.alias_map = defaultdict(set)
        self.recent_speakers = []
        self.mention_regex = {}
        if not ChatParser._nlp_checked:
            try:
                ChatParser._nlp = spacy.load("ko_core_news_sm")
            except Exception:
                logging.warning("ko_core_news_sm 모델이 없음.")
                ChatParser._nlp = None
            ChatParser._nlp_checked = True
        self.nlp = ChatParser._nlp

    def parse_file(self, file_path: str):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for raw in f:
                    line = raw.strip()
                    if not line:
                        continue
                    if m := ChatParser.DATE_PATTERN.match(line):
                        y, mo, d = map(int, m.groups())
                        self.current_date = datetime(y, mo, d)
                    elif m := ChatParser.SYSTEM_PATTERN.match(line):
                        inviter, invited_text = m.groups()
                        invited = [x.strip() for x in invited_text.split(',')]
                        self.speakers.add(inviter)
                        for iv in invited:
                            self.speakers.add(iv)
                        self.invites.append((inviter, invited))
                    elif self.current_date and (m := ChatParser.MESSAGE_PATTERN.match(line)):
                        speaker, period, hh, mm, content = m.groups()
                        timestamp = self._parse_time(self.current_date, period, int(hh), int(mm))
                        self.speakers.add(speaker)
                        self.message_log.append({
                            'speaker': speaker,
                            'time': timestamp,
                            'message': content.strip(),
                            'date': self.current_date
                        })
                        self.recent_speakers.append((speaker, timestamp))
                        self._update_alias_map(speaker, content)
            self._compile_mention_regex()
            logging.info("파일 파싱 완료")
        except Exception as e:
            logging.error(f"파싱 오류: {e}")
            raise

    def _parse_time(self, date_obj: datetime, period: str, hour: int, minute: int) -> datetime:
        if period == '오후' and hour != 12:
            hour += 12
        elif period == '오전' and hour == 12:
            hour = 0
        return datetime(date_obj.year, date_obj.month, date_obj.day, hour, minute)

    def _update_alias_map(self, speaker: str, content: str):
        words = content.split()
        for word in words:
            clean = re.sub(r'[^\w\s가-힣]', '', word).strip()
            if clean and len(clean) >= 2 and clean != speaker:
                self.alias_map[speaker].add(clean)

    def _compile_mention_regex(self):
        for name in self.speakers:
            clean_name = re.sub(r'[^\w\s가-힣]', '', name).strip()
            if not clean_name:
                continue
            aliases = [clean_name] + list(self.alias_map.get(name, []))
            pattern = '|'.join(rf'\b{re.escape(a)}\b' for a in aliases if a)
            self.mention_regex[name] = re.compile(pattern)

    def get_mentions(self, text: str, speaker: str, timestamp: datetime) -> list[str]:
        result = set()
        for other, rx in self.mention_regex.items():
            if other != speaker and rx.search(text):
                result.add(other)
        if self.nlp:
            doc = self.nlp(text)
            for ent in doc.ents:
                if ent.label_ == 'PERSON':
                    for person in self.speakers:
                        if ent.text == person or ent.text in self.alias_map.get(person, []):
                            if person != speaker:
                                result.add(person)
        recent_thresh = timestamp - timedelta(minutes=5)
        recent_set = {s for s, t in self.recent_speakers if t >= recent_thresh}
        if not result and recent_set:
            for rs in recent_set:
                if rs != speaker:
                    result.add(rs)
                    break
        return list(result)

class MsgDataset(Dataset):
    def __init__(self, texts):
        self.texts = texts

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        return self.texts[idx]

class SentimentInteractionAnalyzer:
    def __init__(self, parser: ChatParser, similarity_model):
        self.parser = parser
        self.messages = sorted(parser.message_log, key=lambda x: x['time'])
        self.similarity_model = similarity_model
        self.interactions = defaultdict(lambda: defaultdict(list))
        self.recent_context = []
        self.lda_model = None
        self.lda_dictionary = None
        self._init_lda()

        with open('finetune_dataset.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        self.ref_texts = [item['text'] for item in data]
        self.ref_labels = [item['label'] for item in data]
        self.ref_embs = self.similarity_model.encode(self.ref_texts, convert_to_tensor=True)

    def _init_lda(self):
        if os.path.exists("lda.pkl"):
            self.lda_model, self.lda_dictionary = joblib.load("lda.pkl")
            return

        time_window = timedelta(minutes=10)
        groups, current_group, last_time = [], [], None
        for msg in self.messages:
            if not last_time or msg['time'] - last_time <= time_window:
                current_group.append(msg['message'])
            else:
                groups.append(current_group)
                current_group = [msg['message']]
            last_time = msg['time']
        if current_group:
            groups.append(current_group)

        texts_tokens = []
        for group in groups:
            for text in group:
                toks = extract_nouns(self._preprocess_text(text))
                if toks:
                    texts_tokens.append(toks)

        if texts_tokens:
            self.lda_dictionary = corpora.Dictionary(texts_tokens)
            corpus = [self.lda_dictionary.doc2bow(ts) for ts in texts_tokens]
            self.lda_model = LdaModel(corpus, num_topics=5, id2word=self.lda_dictionary, passes=10)
            joblib.dump((self.lda_model, self.lda_dictionary), "lda.pkl")

    def _preprocess_text(self, text: str) -> str:
        text = emoji.demojize(text, language='ko')
        text = re.sub(r':smile:|:grinning_face:', ' 긍정 ', text)
        text = re.sub(r':cry:|:sad:', ' 부정 ', text)
        text = re.sub(r':\w+:', '', text)
        return text.strip()

    def _get_context_weight(self, timestamp: datetime, text: str) -> float:
        window = timedelta(minutes=5)
        context_msgs = [c for c in self.recent_context if timestamp - c['time'] <= window]
        if not context_msgs:
            return 0.0
        last_weight = context_msgs[-1]['weight']
        if last_weight < -0.5 and any(w in text for w in ['미안', '죄송']):
            return -0.2
        if last_weight > 0.5 and any(w in text for w in ['좋아', '멋져']):
            return 0.2
        if self.lda_model and self.lda_dictionary:
            tokens = extract_nouns(text)
            if tokens:
                bow = self.lda_dictionary.doc2bow(tokens)
                topics = self.lda_model[bow]
                for topic_id, prob in topics:
                    if topic_id == 0 and prob > 0.5:
                        return -0.3
                    if topic_id == 1 and prob > 0.5:
                        return 0.3
        return 0.0

    def analyze(self):
        texts = [self._preprocess_text(m['message']) for m in self.messages]
        dataset = MsgDataset(texts)
        loader = DataLoader(dataset, batch_size=16, num_workers=0)

        idx = 0
        positive_cnt = 0
        total_processed = 0

        for batch in tqdm(loader, desc="유사도 기반 감정분석 진행 중"):
            input_embs = self.similarity_model.encode(batch, convert_to_tensor=True)

            for emb in input_embs:
                cos_scores = util.cos_sim(emb, self.ref_embs)[0]
                top5_idx = torch.topk(cos_scores, k=5).indices
                top5_labels = [self.ref_labels[i] for i in top5_idx]
                final_label = 1 if top5_labels.count(1) > top5_labels.count(0) else 0

                orig_msg = self.messages[idx]['message']
                spkr = self.messages[idx]['speaker']
                ts = self.messages[idx]['time']
                text = batch[idx % len(batch)]

                if final_label == 1:
                    positive_cnt += 1
                    mean_score = cos_scores[top5_idx].mean().item()
                    weight = -mean_score
                    weight += self._get_context_weight(ts, text)

                    self.recent_context.append({'time': ts, 'text': text, 'weight': weight})
                    if len(self.recent_context) > 100:
                        self.recent_context.pop(0)

                    for tgt in self.parser.get_mentions(orig_msg, spkr, ts):
                        self.interactions[spkr][tgt].append((ts, weight, orig_msg))

                idx += 1
                total_processed += 1

        return self.interactions

    def detect_persistent_attacks(self, min_attacks=5, min_days=1) -> list[tuple]:
        cases = []
        for sender, targets in self.interactions.items():
            for receiver, events in targets.items():
                neg_events = [(t, w, m) for t, w, m in events if w < 0]
                if len(neg_events) >= min_attacks:
                    duration = (max(t for t, _, _ in neg_events) - min(t for t, _, _ in neg_events)).days
                    if duration >= min_days:
                        msg_list = [f"{t.strftime('%Y-%m-%d %H:%M:%S')}: {m}" for t, _, m in neg_events]
                        cases.append((sender, receiver, len(neg_events), duration, msg_list))

        print(f"[STEP4] 지속적 공격 사례 수: {len(cases)}")

        return cases

    def save_persistent_cases_to_csv(self, output_path="persistent_cases.csv"):
        cases = self.detect_persistent_attacks()
        try:
            with open(output_path, 'w', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(["가해자", "피해자", "부정 메시지 수", "지속 일수", "부정 메시지 내용"])
                for case in cases:
                    writer.writerow(case)
            logging.info(f"지속적 공격 사례를 {output_path}에 저장 완료")
        except Exception as e:
            logging.error(f"CSV 저장 실패: {e}")
            raise

    def visualize_persistent_cases(self, png_output_path="persistent_cases_graph.png", html_output_path="persistent_cases_graph.html"):
        G = nx.DiGraph()
        cases = self.detect_persistent_attacks()
        for sender, receiver, count, duration, _ in cases:
            neg_weights = [w for t, w, m in self.interactions[sender][receiver] if w < 0]
            avg_neg_score = abs(sum(neg_weights) / len(neg_weights)) if neg_weights else 0
            G.add_edge(sender, receiver, weight=count, duration=duration, avg_neg_score=avg_neg_score)

        node_sizes = {n: max(500, sum(len(self.interactions[n][t]) for t in self.interactions[n]) * 200) for n in G.nodes()}
        pos = nx.spring_layout(G, k=2.5, iterations=100, seed=42)

        try:
            sns.set_theme(style='whitegrid')
        except Exception:
            plt.style.use('default')
        plt.figure(figsize=(14, 12), facecolor='white')
        nx.draw_networkx_nodes(G, pos, node_color='white', edgecolors='black', linewidths=2, node_size=[node_sizes[n] for n in G.nodes()])
        nx.draw_networkx_labels(G, pos, font_size=14, font_weight='bold')
        edges = G.edges(data=True)
        weights = [d['weight'] for _, _, d in edges]
        max_w = max(weights) if weights else 1
        widths = [3 * (d['weight'] / max_w) + 1 for _, _, d in edges]
        scores = [d['avg_neg_score'] for _, _, d in edges]
        norm = colors.Normalize(vmin=min(scores) if scores else 0, vmax=max(scores) if scores else 1)
        edge_colors = plt.cm.Reds(norm(scores))
        nx.draw_networkx_edges(G, pos, edgelist=G.edges(), width=widths, edge_color=edge_colors, arrows=True, arrowsize=30, connectionstyle='arc3,rad=0.2')
        labels = {(u, v): f"{d['weight']}회\n{d['duration']}일" for u, v, d in edges}
        nx.draw_networkx_edge_labels(G, pos, edge_labels=labels, font_size=10, bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))
        sm = plt.cm.ScalarMappable(cmap=plt.cm.Reds, norm=norm)
        plt.colorbar(sm, label='평균 부정 스코어', ax=plt.gca())
        plt.title(f"지속적 사이버불링 관계 시각화 - 사용자 {len(G.nodes())}명, 관계 {len(G.edges())}개", fontsize=16, pad=20)
        plt.axis('off')
        plt.tight_layout()
        try:
            plt.savefig(png_output_path, dpi=300, bbox_inches='tight')
            logging.info(f"PNG 그래프를 {png_output_path}에 저장 완료")
        except Exception as e:
            logging.error(f"PNG 저장 실패: {e}")
        plt.close()

        try:
            node_x, node_y, node_text = [], [], []
            edge_x, edge_y = [], []
            for node in G.nodes():
                x, y = pos[node]
                node_x.append(x)
                node_y.append(y)
                node_text.append(node)

            for u, v, d in G.edges(data=True):
                x0, y0 = pos[u]
                x1, y1 = pos[v]
                edge_x += [x0, x1, None]
                edge_y += [y0, y1, None]

            node_trace = go.Scatter(
                x=node_x, y=node_y, mode='markers+text', text=node_text,
                textposition='middle center', hoverinfo='text',
                marker=dict(showscale=False, color='white', line=dict(color='black', width=2), size=[node_sizes[n] / 50 for n in G.nodes()])
            )
            edge_trace = go.Scatter(
                x=edge_x, y=edge_y, line=dict(width=1, color='rgba(200,0,0,0.4)'),
                hoverinfo='none', mode='lines'
            )

            annotations = []
            for u, v, d in G.edges(data=True):
                x0, y0 = pos[u]
                x1, y1 = pos[v]
                annotations.append(dict(
                    x=x1, y=y1, ax=x0, ay=y0, xref='x', yref='y', axref='x', ayref='y',
                    showarrow=True, arrowhead=1, arrowsize=1, arrowwidth=1, arrowcolor='crimson', opacity=0.9
                ))
                mid_x = (x0 + x1) / 2
                mid_y = (y0 + y1) / 2
                annotations.append(dict(
                    x=mid_x, y=mid_y, xref='x', yref='y', text=f"{d['weight']}회 / {d['duration']}일",
                    showarrow=False, font=dict(size=10, color='black'), bgcolor='rgba(255,255,255,0.7)', borderpad=2
                ))

            layout = go.Layout(
                title=dict(text=f"<b>지속적 사이버불링 관계 시각화</b><br>사용자 {len(G.nodes())}명, 관계 {len(G.edges())}개", font=dict(size=18)),
                showlegend=False, hovermode='closest', margin=dict(b=20, l=5, r=5, t=60),
                annotations=annotations, xaxis=dict(showgrid=False, zeroline=False), yaxis=dict(showgrid=False, zeroline=False)
            )
            fig = go.Figure(data=[edge_trace, node_trace], layout=layout)
            fig.write_html(html_output_path)
            logging.info(f"HTML 그래프를 {html_output_path}에 저장 완료")
        except Exception as e:
            logging.error(f"HTML 저장 실패: {e}")

class CyberbullyingStatementGenerator:
    def __init__(self, csv_path: str, png_path: str, html_path: str):
        self.csv_path = Path(csv_path)
        self.png_path = Path(png_path)
        self.html_path = Path(html_path)
        self.cases = []
        self.model_id = None
        self.load_cases()

    def load_cases(self):
        try:
            with open(self.csv_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    self.cases.append({
                        'attacker': row['가해자'],
                        'victim': row['피해자'],
                        'count': int(row['부정 메시지 수']),
                        'duration': int(row['지속 일수']),
                        'messages': eval(row['부정 메시지 내용'])
                    })
            logging.info(f"{len(self.cases)}개의 지속적 공격 사례 로드 완료")
        except Exception as e:
            logging.error(f"CSV 로드 실패: {e}")
            raise

    # def prepare_finetune_data(self, output_path: str):
    #     try:
    #         with open(output_path, 'w', encoding='utf-8') as f:
    #             for data in LEGAL_DATA:
    #                 json.dump({
    #                     "messages": [
    #                         {"role": "user", "content": data["prompt"]},
    #                         {"role": "assistant", "content": data["completion"]}
    #                     ]
    #                 }, f, ensure_ascii=False)
    #                 f.write('\n')
    #         logging.info(f"파인튜닝 데이터 {output_path}에 저장 완료")
    #     except Exception as e:
    #         logging.error(f"파인튜닝 데이터 준비 실패: {e}")
    #         raise

    # def finetune_model(self, training_file: str):
    #     try:
    #         with open(training_file, 'rb') as f:
    #             file_response = client.files.create(file=f, purpose="fine-tune")
    #         finetune_response = client.fine_tuning.jobs.create(
    #             training_file=file_response.id,
    #             model="gpt-3.5-turbo",
    #             hyperparameters={"n_epochs": 3}
    #         )
    #         self.model_id = finetune_response.fine_tuned_model
    #         logging.info(f"파인튜닝 작업 시작: {finetune_response.id}")
    #         logging.info(f"파인튜닝 완료, 모델 ID: {self.model_id}")
    #     except Exception as e:
    #         logging.error(f"파인튜닝 실패: {e}")
    #         raise

    def generate_statement(self, output_path: str):
        try:
            case_summary = "\n".join([
                f"가해자: {self.cases[0]['attacker']}, 피해자: {self.cases[0]['victim']}, "
                f"부정 메시지 수: {self.cases[0]['count']}, 지속 일수: {self.cases[0]['duration']}, "
                f"메시지 예시: {self.cases[0]['messages'][:2]}"
            ])
            png_description = (
                f"네트워크 그래프(PNG: {self.png_path})는 가해자와 피해자 간 부정적 상호작용을 시각화하며, "
                f"엣지 두께는 메시지 빈도, 색상은 부정 스코어를 나타냅니다."
            )
            prompt = ("""당신은 대한민국 법률과 판례에 정통한 법률 전문가입니다.

                        진술서는 법적 증거로 활용할 수 있어야 하며,
                        정보통신망 이용촉진 및 정보보호 등에 관한 법률, 형법, 성폭력처벌법, 개인정보보호법, 학교폭력예방법 등을 고려하여
                        구체적 법률 조항과 실제 판례를 가능한 한 인용해 주세요.

                        진술서는 다음과 같은 구조를 따라야 합니다:

                        1. 서론

                        3. 법적 분석

                        4. 결론

                        """) + f"{case_summary}" # + {png_description}

            # 토큰을 st.secrets에서 가져오기
            API_TOKEN = st.secrets["huggingface"]["api_token"]

            API_URL = "https://api-inference.huggingface.co/models/facebook/opt-1.3b"

            headers = {
                "Authorization": f"Bearer {API_TOKEN}",
                "Content-Type": "application/json"
            }               

            payload = {
                "inputs": prompt,
                "parameters": {
                    "temperature": 0.7,
                    "max_new_tokens": 256,
                    "top_p": 0.9,
                    "repetition_penalty": 1.1
                }
            }

            response = requests.post(API_URL, headers=headers, json=payload)
            print(response.text)
            if response.status_code == 200:
                output = response.json()
                full_text = output[0]['generated_text']

                generated_text = full_text[len(prompt):].strip()

                with open(output_path, "w", encoding="utf-8") as f:
                    f.write(generated_text)

                return generated_text
            else:
                print(f"Error: {response.status_code}")
                print(response.text)
                return None

        except Exception as e:
            logging.error(f"진술서 생성 실패: {e}")
            raise

    def deliver_to_victim(self):
        """피해자에게 결과 파일 전달 (간소화된 출력)"""
        try:
            message = (
                "피해자님, 다음 파일을 확인해주세요:\n"
                f"- CSV: {self.csv_path}\n"
                f"- PNG: {self.png_path}\n"
                f"- HTML (인터랙티브 그래프): {self.html_path}\n"
            )
            print(message)
            logging.info("피해자에게 결과 파일 전달 메시지 출력")
        except Exception as e:
            logging.error(f"피해자 전달 실패: {e}")
            raise


def main():
    st.title("사이버불링 진단 시스템")

    uploaded_file = st.file_uploader("카카오톡 TXT 파일을 업로드하세요", type=['txt'])

    if uploaded_file is not None:
        # 파일 저장
        temp_path = "uploaded_kakao_chat.txt"
        with open(temp_path, "wb") as f:
            f.write(uploaded_file.getbuffer())


        # 모델 준비
        similarity_model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')

        # 파싱 시작
        parser = ChatParser()
        parser.parse_file(temp_path)

        analyzer = SentimentInteractionAnalyzer(parser, similarity_model)
        analyzer.analyze()
        analyzer.save_persistent_cases_to_csv('persistent_cases.csv')
        analyzer.visualize_persistent_cases('persistent_cases_graph.png', 'persistent_cases_graph.html')

        # 진술서 생성
        statement_generator = CyberbullyingStatementGenerator(
            csv_path='persistent_cases.csv',
            png_path='persistent_cases_graph.png',
            html_path='persistent_cases_graph.html'
        )
        statement_generator.generate_statement("cyberbullying_statement.md")
        statement_generator.deliver_to_victim()

        # 파일 다운로드 버튼 만들기
        st.success("분석 완료! 결과 파일을 다운로드하세요.")
        
        with open("persistent_cases.csv", "rb") as f:
            st.download_button("CSV 파일 다운로드", f, file_name="persistent_cases.csv", mime="text/csv")

        with open("persistent_cases_graph.png", "rb") as f:
            st.download_button("그래프 PNG 다운로드", f, file_name="persistent_cases_graph.png", mime="image/png")

        with open("persistent_cases_graph.html", "rb") as f:
            st.download_button("인터랙티브 그래프 HTML 다운로드", f, file_name="persistent_cases_graph.html", mime="text/html")

        with open("cyberbullying_statement.md", "rb") as f:
            st.download_button("진술서(MD 파일) 다운로드", f, file_name="cyberbullying_statement.md", mime="text/markdown")

if __name__ == "__main__":
    main()

